# 경량 딥러닝

- 경량 딥러닝에 대한 조사내용



## 서론

- 경량 딥러닝이란, 기존의 학습된 모델의 정확도를 유지하면서 보다 크기가 작고, 연산을 간소화하는 연구

- 지연시간 감소, 민감한 개인 정보 보호, 네트워크 트래픽 감소와 같은 이점을 갖을 수 있다.

- 경량 딥러닝의 종류 : 

  1. 경량 딥러닝 알고리즘

     - 알고리즘 자체를 적은 연산과 효율적인 구조로 설계하여, 기존 모델 대비 효율을 극대화

     - 모델구조 변경, 합성곱 필터 변경, 자동 모델 탐색

       

  2. 알고리즘 경량화

     - 만들어진 모델의 파라미터들을 줄이는 모델 압축(Model Compression) 등의 기법이 적용
     - 모델 압축, 지식 증류, 하드웨어 가속화, 모델 압축 자동 탐색

  



## 경량화 기술

### 1. 경량 딥러닝 알고리즘

가장 일반화된 CNN을 통해 다양한 연구가 진행되고 있습니다. 

 

#### 1) 모델 구조 변경

단일 층별 연산에서 그치지 않고 잔여 블록, 병목구조, 밀집 블록 등과 같은 형태를 반복적으로 쌓아 신경망으로 구성하는 다양한 신규 계층 구조를 이용하여 파라미터 축소 및 모델 성능을 개선하는 연구가 진행되고 있습니다.

 

**- ResNet**

 

**- DenseNet**

 기존 신경망 모델 구조의 여러 장점을 모아 고안한 모델입니다.

 기존 피쳐맵을 더해주는 게 아닌 쌓아가는 과정을 거치며 모델의 성능을 높이고자 하였습니다.

 

**- SqueezeNet**

 기본적으로 사용하는 합성곱 필터인 3X3 필터를 1X1 필터로 대체하여 9배 적은 파라미터를 가집니다.
 1X1 합성곱을 이용하여 채널 수를 줄였다가 다시 늘리는 파이어 모듈(Fire Module) 기법을 제안하였습니다.
 늦은 다운 샘플링 전략을 통해 한번에 필터가 볼 수 있는 영역을 좁히면서 해당 이미지의 정보를 압축시키는 효과가 있습니다.

 

#### 2) 합성곱 필터 변경

이 기술에 대한 연구가 주로 이루어지고 있다고 합니다.

합성곱 신경망의 가장 큰 계산량을 요구하는 합성곱 필터의 연산을 효율적으로 줄이는 연구입니다. 

모델 구조를 변경하는 다양한 경량 딥러닝 기법은 점차 채널을 분리하여 학습시키면서 연산량과 변수의 개수를 줄일 수 있는 연구로 확장되었습니다.
일반적인 합성곱은 채널 방향으로 모두 연산을 수행하여 하나의 특징을 추출하는 데 반해, 채널별(Depthwise)로 합성곱을 수행하고, 다시 점별(Pointwise)로 연산을 나누어 전체 파라미터를 줄이는 것과 같이 다양한 합성곱 필터를 설계하기 시작하였습니다.

 

**- MobileNet**

 기존의 합성곱 필터를 채널 단위로 먼저 합성곱(Depthwise Convolution)을 하고, 그 결과를 하나의 픽셀(Point)에 대하여 진행하는 합성곱(Pointwise Convolution)으로 나눔으로써 한 예로, 필터의 가로, 세로 길이를 3이라고 할때, 약 8~9배의 이득이 있게 한 네트워크입니다.

 

**- ShuffleNet**

 점별 합성곱(Pointwise convolution)시 특정 영역의 채널에 대해서만 연산을 취하는 형태로 설계하면 연산량을 매우 줄일 수 있을 것이란 아이디어에서 출발하였습니다. 입력에서만의 정보 흐름만을 취하는 대신, 입력의 각 그룹이 잘 섞일 수 있도록 개선한 것이 핵심입니다.

 

#### 3) 자동 모델 탐색

기존 신경망의 모델 구조를 인간에게 의존적으로 수행하지 않고 모델 구조를 자동 탐색하는 기술들에 대해 연구가 이루어지고 있다고 합니다. 특정 요소(지연시간, 에너지 소모 등)가 주어진 경우, 강화 학습을 통해 최적 모델을 자동 탐색하는 연구입니다.

 

**- NetAdapt(넷어덥트)**

 

**- MnasNet(엠나스넷)**

 

 

 

### 2. 알고리즘 경량화

기존 알고리즘의 불필요한 파라미터를 제거하거나, 파라미터의 공통된 값을 가지고 공유하거나, 파라미터의 표현력을 잃지 않으면서 기존 모델의 크기를 줄이는 연구 분야입니다.

 

#### 1) 모델 압축 기술

 

**- Pruning**

 기존 신경망이 가지고 있는 가중치 중 작은 가중치값을 모두 0으로 하여 네트워크의 모델 크기를 줄이는 기술입니다.



![img](https://blog.kakaocdn.net/dn/BjDGN/btqGcfXFNPP/gt4exNu7qaMV7EJSVVxKSK/img.png)



 

**- Quantization / Binarization**

  Quantization은 특정 비트 수만큼으로 줄여서 계산하는 방식입니다. 파라미터의 Precision을 적절히 줄여서 연산 효율성을 높이는 방법입니다. 16bit, 8bit 이런식으로 줄여서 사용합니다.

 

**- Weight Sharing**

  낮은 정밀도에 대한 높은 내성을 가진 신경망의 특징을 활용해 가중치를 근사하는 방법입니다.

 

#### 2) 지식 증류 기술 (Knowledge Distillation)

앙상블 기법을 통해 학습된 다수의 큰 네트워크로부터 작은 하나의 네트워크에 지식을 전달할 수 있는 방법론 중의 하나입니다.

Teacher 모델을 Student 모델 학습에 활용합니다.

 

#### 3) 하드웨어 가속화 기술 

벡터/행렬 연산을 병렬 처리하기 위한 전용 하드웨어 (TPU), VPU, GPU Custer 기반 가속기 등의 연구개발이 주요 IT 기업에 의해 주도되고 있다고 합니다.

 

#### 4) 모델 압축을 적용한 경량 모델 자동 탐색 기술

Pruning, Quantization 등의 탐색 공간을 통한 자동화 연구가 진행되고 있습니다.





## 정리

#### 국내동향

- 기술
  - 양자화 구간 학습 기술기반의 경량 AI 분석 기술(알고리즘 경량화)
  - AI 성능 최적화 - 경량화 네트워크, 연산엔진 최적화
  - 고성능 AI 모델 경량화 기술 플랫폼 - AI 딥러닝 모델을 압축, 가지치기 기법, 양자화 기법
  - 4-bit 이하 양자화 기술 
  - 8 bits 가지치기
  - 가지치기(Pruning), 양자화(Quantization), 증류(Distillation), 디바이스의 최소 자원 사용
  - 양자화 인식 재훈련 - 손실된 성능을 재훈련을 통해 복구
  - 
- 장점
  - 빠른 AI와 낮은 전력 소비
  - 빠르고 
  - 처리속도
  - AI 모델의 시각화, 경량화된 모델의 속도 및 정확도 비교 분석 가능
  - 전력사용량, 연산속도, 열화도를 1% 이내로 제한
  - 
- 활용
  - 스마트폰용 칩, 메모리, 센서
  - 웹 서비스
  - 에지 디바이스에서 사용할 수 있도록 전력사용량, 연산량, 속도 등의 이점



- 정리
  - 국내 많은 업체들이 양자화, 가지치기, 지식증류 등의 기법을 활용해 고성능 AI 분석 기술을 경량화 시키기 위한 기술들을 개발중이며 스마트폰, 엣지 디바이스 등 소형 단말에 적용이 가능함



- 단점
  - 학습 비용
    - 네이버, 삼성 등 대형 기업들도 개발중이지만 스타트업 같은 소기업에서도 활발히 개발중이다 하지만 학습 비용이 높아 다양한 방면에서 지원 혹은 협업이 필요할 것으로 보임
  - 모델의 성능도 유저가 원하는 수준에 다다르지 못했
  - 정보의 손실
  - 인공지능 모델을 경량화하면 모델의 정확도가 저하될 가능성이 있다.
  - 정리
    - AI 경량화 연구는 활발히 진행중이지만 알고리즘 경량화에 따른 정보의 손실이 일어나고 그에 따른 AI 모델의 정확도가 저하될 가능성이 있음





#### 국내동향

- 기술
  - 모델을 경량화하고, 프레임 간 호환 - 개별 프레임워크에서 생성된 모델을 그래프화하여 구조화 후 양자화 등의 기법을 통해 경량화
  - AI 모델을 경량화하여 하드웨어 가속 및 모델 최적화를 제공
  - 양자화 및 정밀도 캘리브레이션, 그래프 최적화, 커널 자동 튜닝, 동적 텐서 메모리 및 멀티 스트림 실행
  - 모델 압축 및 양자화 기술 - 고급 모델 압축 방법으로 SVD(support vector machine), 가지치기, 네트워크 계층 비율 압축 기법 / 양자화 방법으로 Cross-Layer Equalization, Bias Correction, Adaptive Rounding 사용 
  - offline conversion은 양자화 등의 기법을 적용하여 모델 경량화를 수행하고, on-device inference에서는 하드웨어 최적화를 수행, Tensorflow, Tensorflow Lite 등과 연동
  - int8, bfloat16 또는 FP32, BF16 및 int8의 혼합으로 양자화하여 정밀도 손실을 최소화하고 모델 크기를 줄이여 추론 속도를 높임. 파라미터를 가지치기하여 네트워크 크기를 줄이고 구조화 또는 구조화되지 않은 희소성 패턴의 가중치를 버리거나 지정된 규칙에 따라 필터 또는 계층을 제거함, 지식 증류, 그래프 최적화 



- 장점
  - 변환 중에 최적화를 적용하여 정확성의 손실이 없거나 최소화된 상태로 모델 크기와 지연시간을 줄임



- 활용
  - 모바일
  - 엣지 디바이스
  - 저사양 기기



- 정리
  - 



#### 단점

- 